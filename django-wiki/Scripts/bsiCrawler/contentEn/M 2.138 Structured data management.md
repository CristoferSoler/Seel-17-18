#M 2.138 Structured data management
Responsible for initiation: IT security officer, IT manager

Responsible for implementation: administrator, user

A poorly structured data management can lead to a variety of problems. All IT users should therefore be advised what a well-structured and clear data management should look like. On all servers corresponding structures should be specified by the administrators. This is a prerequisite anyway in order to realize a differentiated allocation of access rights.

Program and work files should always be stored in separate areas. This provides a better overview and also facilitates the execution of data backups and ensuring the correct access protection. Most application programs do not change any or very few configuration files after the installation. As far as possible, all files that change regularly should be stored in separate directories, so that only these have to be included in the regular backups.

With a clear separation of programs and data, it is sufficient to include the data in the regular backups. It is important to have the work files carefully backed up, these can then be processed on other systems if necessary.

For networked systems, there is also the question of which programs or files should be stored on the local hard disks or on a network server. Both have advantages and disadvantages and must be dependent both on the organizational structure and on the hardware and software used. So should z. B. Files with high availability requirements, together with the associated application programs, are better kept on the workstations than on a network server. Then, however, the corresponding emergency preparedness for these workstations must be operated.

Task- or project-related directories should be set up to facilitate the allocation of files. As little data as possible should be stored in personal directories.

In order to prevent basic files such as letter templates, forms, project plans or similar versions from existing for further work, these should be managed centrally. For example, they should be kept on a server so that anyone can read it, but there should only be one person for each such file to modify it.

The following example shows how data could be structured on a server through directory defaults:

/ / bin / bin / program1 / bin / program2 / bin / program3 / user / user / user1 / user / user2 / projects / projects / p1 / projects / p1 / texts / projects / p1 / images / projects / p2 / projects / p2 / project plan / projects / p2 / subproject1 / projects / p2 / subproject2 / projects / p2 / subproject3 / projects / p2 / result / impressions

It should be checked regularly,

* whether data can be removed from the production system because it can be archived or deleted,
* whether access rights can be revoked because employees have left the project group,
* Whether the latest versions of forms, templates, etc. are stored on all IT systems.


This must be checked regularly by the users for their IT systems or the directories managed by them and by the administrators of the servers. These tests should be carried out at least once a quarter, otherwise the knowledge about content and origin of the files will disappear again from the memory of the employees.



